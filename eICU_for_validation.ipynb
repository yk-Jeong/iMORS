{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "import re\n",
    "from datetime import datetime, timedelta, time\n",
    "\n",
    "# scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap \n",
    "\n",
    "# dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler, SequentialSampler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# wandb\n",
    "import wandb\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import log_loss, roc_auc_score, roc_curve, average_precision_score, accuracy_score, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc as calculate_auc  \n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL 서버 연결 정보\n",
    "\n",
    "host = '192.168.0.76'  # 호스트 주소\n",
    "database = 'eicu'  # 데이터베이스 이름\n",
    "user = 'ykjeong'  # 사용자 이름\n",
    "password = 'mdhi1234!'  # 비밀번호\n",
    "\n",
    "# PostgreSQL 서버에 연결\n",
    "conn = psycopg2.connect(\n",
    "    dbname=database,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    host=host\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_635957/2270477220.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  patients = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT\n",
    "    patientunitstayid AS stay_id,\n",
    "    gender,\n",
    "    age,\n",
    "    ethnicity AS race,\n",
    "    unitType AS CU_type,\n",
    "    unitAdmitTime24 AS intime, \n",
    "    unitDischargeTime24 AS death_or_dischtime,\n",
    "    unitDischargeStatus AS dead_in_hosp\n",
    "FROM\n",
    "    eicu.patient\n",
    "WHERE -- 1일 이상 60일 미만 입원한 19세 이상의 환자\n",
    "    unitDischargeOffset / 1440.0 >= 1 \n",
    "    AND unitDischargeOffset / 1440.0 < 60;\n",
    "'''\n",
    "patients = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.rename(columns={\n",
    "    'cu_type':'CU type', \n",
    "    'dead_in_hosp':'dead in hosp',\n",
    "    'death_or_dischtime':'death or dischtime'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'age' 컬럼 핸들링 \n",
    "# '> 89'를 91로 처리\n",
    "patients.loc[patients['age'] == '> 89', 'age'] = 91\n",
    "\n",
    "# 결측치를 평균으로 대체\n",
    "patients['age'] = pd.to_numeric(patients['age'], errors='coerce')\n",
    "patients['age'] = patients['age'].fillna(patients['age'].mean())\n",
    "patients['age'] = patients['age'].astype(int)\n",
    "\n",
    "# 18세 이상만 추출 \n",
    "patients = patients[patients['age'] > 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MICU', 'other', 'Neuro ICU', 'SICU', 'CCU'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorize_careunit(careunit):\n",
    "    if 'MICU' in careunit or'Med-Surg ICU' in careunit:\n",
    "        return 'MICU'\n",
    "    elif 'SICU' in careunit:\n",
    "        return 'SICU'\n",
    "    elif 'Neuro ICU' in careunit:\n",
    "        return 'Neuro ICU'\n",
    "    elif 'Cardiac ICU' in careunit or 'CSICU' in careunit:\n",
    "        return 'CCU'\n",
    "    elif 'CVICU' in careunit:\n",
    "        return 'CVICU'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "patients['CU type'] = patients['CU type'].apply(categorize_careunit)\n",
    "patients['CU type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['white', 'other', 'black', 'hispanic/latino'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인종 분류 및 결측치 수정 \n",
    "\n",
    "def categorize_race(race):\n",
    "    if 'Caucasian' in race:\n",
    "        return 'white'\n",
    "    elif 'African American' in race:\n",
    "        return 'black'\n",
    "    elif 'Hispanic' in race:\n",
    "        return 'hispanic/latino'\n",
    "    elif 'ASIAN' in race:\n",
    "        return 'asian'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "patients['race'] = patients['race'].apply(categorize_race)\n",
    "patients['race'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dead in hosp\n",
       "0    0.947866\n",
       "1    0.052134\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dead in hosp 컬럼의 값을 변경하고 비율 확인\n",
    "\n",
    "patients.loc[:, 'dead in hosp'] = patients['dead in hosp'].map(lambda x: 1 if x == 'Expired' else 0)\n",
    "patients['dead in hosp'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dead in hosp\n",
       "0    0.784312\n",
       "1    0.215688\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stay_id 분리\n",
    "\n",
    "patients_pos_samp = patients[patients['dead in hosp'] == 1]\n",
    "\n",
    "# 'dead in hosp'가 0인 행들에서 50%만 랜덤으로 추출\n",
    "patients_neg_samp = patients[patients['dead in hosp'] == 0].sample(frac=0.2, random_state=42)\n",
    "\n",
    "patients = pd.concat([patients_neg_samp, patients_pos_samp])\n",
    "patients['dead in hosp'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.to_parquet('eICU_patients.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_593400/2683277839.py:41: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  lab = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    lab.patientunitstayid AS stay_id,\n",
    "    lab.labname AS label,\n",
    "    lab.labresulttext AS value,\n",
    "    lab.labTypeID AS labtype, \n",
    "    lab.labmeasurenamesystem AS valueoum, \n",
    "    lab.labresultoffset / 60 AS time\n",
    "FROM\n",
    "    eicu.lab lab\n",
    "WHERE(\n",
    "        lab.labname LIKE '%AST%' OR\n",
    "        lab.labname LIKE '%ALT%' OR\n",
    "        lab.labname LIKE '%albumin%' OR\n",
    "        lab.labname LIKE '%bilirubin%' OR\n",
    "        lab.labname LIKE '%BUN%' OR\n",
    "        lab.labname LIKE '%chloride%' OR\n",
    "        lab.labname LIKE '%CRP%' OR\n",
    "        lab.labname LIKE '%glucose%' OR\n",
    "        lab.labname LIKE '%Hgb%' OR\n",
    "        lab.labname LIKE '%respiratory%' OR\n",
    "        lab.labname LIKE '%platelet%' OR\n",
    "        lab.labname LIKE '%potassium%' OR\n",
    "        lab.labname LIKE '%Temperature%' OR\n",
    "        lab.labname LIKE '%urinary sodium%' OR\n",
    "        lab.labname LIKE '%urinary creatinine%' OR\n",
    "        lab.labname LIKE '%WBC x 1000%' OR\n",
    "        lab.labname LIKE '%PT%' AND \n",
    "        lab.labmeasurenamesystem LIKE '%sec%' \n",
    "    )\n",
    "    AND lab.patientunitstayid IN (\n",
    "        SELECT\n",
    "            patient.patientunitstayid\n",
    "        FROM\n",
    "            eicu.patient patient\n",
    "        WHERE\n",
    "            patient.unitDischargeOffset / 1440.0 >= 1 \n",
    "            AND patient.unitDischargeOffset / 1440.0 < 60\n",
    "    );\n",
    "'''\n",
    "lab = pd.read_sql_query(query, conn)\n",
    "lab = lab[lab['stay_id'].isin(patients['stay_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab['label']의 값이 'glucose - CSF', 'prealbumin', 'direct bilirubin', 'CRP-hs'인 행 삭제 \n",
    "\n",
    "values_to_remove = ['glucose - CSF', 'prealbumin', 'direct bilirubin', 'CRP-hs', 'PTT']\n",
    "lab = lab[~lab['label'].isin(values_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab['label'] = lab['label'].replace({\n",
    "    r' \\(SGPT\\)': '',\n",
    "    r' \\(SGOT\\)': '',\n",
    "    r'urinary ': '',\n",
    "    r' x 1000': '',\n",
    "    r'bedside ': '',\n",
    "    r'total ': ''\n",
    "}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value 열의 모든 특수문자를 제거하고 수치형으로 변환\n",
    "\n",
    "lab['value'] = lab['value'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "lab['value'] = pd.to_numeric(lab['value'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay_id</th>\n",
       "      <th>label</th>\n",
       "      <th>value</th>\n",
       "      <th>labtype</th>\n",
       "      <th>valueoum</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141168</td>\n",
       "      <td>PT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>sec</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141168</td>\n",
       "      <td>creatinine</td>\n",
       "      <td>11.171875</td>\n",
       "      <td>4.0</td>\n",
       "      <td>mg/dL</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141168</td>\n",
       "      <td>BUN</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>mg/dL</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141168</td>\n",
       "      <td>sodium</td>\n",
       "      <td>127.740560</td>\n",
       "      <td>4.0</td>\n",
       "      <td>mmol/L</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141168</td>\n",
       "      <td>PT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>sec</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stay_id       label       value  labtype valueoum  time\n",
       "0   141168          PT         NaN      3.0      sec    18\n",
       "1   141168  creatinine   11.171875      4.0    mg/dL    28\n",
       "2   141168         BUN   26.000000      1.0    mg/dL     8\n",
       "3   141168      sodium  127.740560      4.0   mmol/L    28\n",
       "4   141168          PT         NaN      3.0      sec     3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vital sign 이상치 조정 \n",
    "\n",
    "ranges = {           \n",
    "    'ALT':(0, 5000), \n",
    "    'AST':(0, 10000), \n",
    "    'albumin':(0, 5.5), \n",
    "    'BUN':(0, 150), \n",
    "    'bilirubin':(0, 50),\n",
    "    'CRP':(0, 300), \n",
    "    'chloride':(70, 135),\n",
    "    'creatinine':(0, 15),\n",
    "    'glucose':(0, 600),\n",
    "    'Hgb':(0, 25),\n",
    "    'Temperature': (32, 41), \n",
    "    'potassium':(2, 9),\n",
    "    'sodium':(105, 170),\n",
    "    'platelets':(0, 1000),\n",
    "    'PT':(0, 8), \n",
    "    'WBC':(0, 90)}\n",
    "\n",
    "# 범위를 벗어난 값들을 NaN으로 대체\n",
    "def replace_out_of_range_with_nan(row):\n",
    "    label = row['label']\n",
    "    value = row['value']\n",
    "    \n",
    "    if label in ranges:\n",
    "        lower, upper = ranges[label]\n",
    "        \n",
    "        # 값이 범위를 벗어나면 NaN으로 대체\n",
    "        if value < lower or value > upper:\n",
    "            return np.nan\n",
    "    return value\n",
    "\n",
    "lab['value'] = lab.apply(replace_out_of_range_with_nan, axis=1)\n",
    "\n",
    "# NaN 값을 각 label에 대한 평균값으로 대체\n",
    "lab['value'] = lab.groupby('label')['value'].transform(lambda x: x.fillna(x.mean()))\n",
    "lab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.to_parquet('eICU_lab.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vital aperiodic: sbp, dbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = pd.read_parquet('eICU_patients.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_593400/1725982474.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  vitalaPeriodic = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT\n",
    "    vitala.patientunitstayid AS stay_id,\n",
    "    vitala.observationoffset / 60 AS time, \n",
    "    vitala.noninvasivesystolic AS sbp,\n",
    "    vitala.noninvasivediastolic AS dbp\n",
    "FROM\n",
    "    eicu.vitalaPeriodic vitala\n",
    "WHERE\n",
    "    vitala.patientUnitStayID IN (\n",
    "        SELECT\n",
    "            patient.patientunitstayid\n",
    "        FROM\n",
    "            eicu.patient patient\n",
    "        WHERE\n",
    "            patient.unitDischargeOffset / 1440.0 >= 1 \n",
    "            AND patient.unitDischargeOffset / 1440.0 < 60\n",
    "    );\n",
    "'''\n",
    "vitalaPeriodic = pd.read_sql_query(query, conn)\n",
    "vitalaPeriodic = vitalaPeriodic[vitalaPeriodic['stay_id'].isin(patients['stay_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitalap = pd.melt(vitalaPeriodic, id_vars=['stay_id', 'time'], value_vars=['sbp', 'dbp'],\n",
    "                    var_name='label', value_name='value')\n",
    "vitalap = vitalap[['stay_id', 'label', 'value', 'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vital sign 이상치 조정 \n",
    "\n",
    "# 기준 설정\n",
    "ranges = {\n",
    "    'sbp': (40, 230),\n",
    "    'dbp': (20, 130)\n",
    "}\n",
    "\n",
    "\n",
    "# 범위를 벗어난 값들을 NaN으로 대체\n",
    "def replace_out_of_range_with_nan(row):\n",
    "    label = row['label']\n",
    "    value = row['value']\n",
    "    \n",
    "    if label in ranges:\n",
    "        lower, upper = ranges[label]\n",
    "        \n",
    "        # 값이 범위를 벗어나면 NaN으로 대체\n",
    "        if value < lower or value > upper:\n",
    "            return np.nan\n",
    "    return value\n",
    "\n",
    "vitalap['value'] = vitalap.apply(replace_out_of_range_with_nan, axis=1)\n",
    "vitalap['value'] = vitalap.groupby('label')['value'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitalap.to_parquet('eICU_vitalap.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nurse charting : Respiratory Rate, Heart rate, sbp, dbp, temperature, SpO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    chart.patientunitstayid AS stay_id,\n",
    "    chart.nursingChartCellTypeValName AS label,\n",
    "    chart.nursingChartValue AS value,\n",
    "    chart.nursingChartEntryOffset / 60 AS time\n",
    "FROM\n",
    "    eicu.nurseCharting chart\n",
    "WHERE (\n",
    "        chart.nursingChartCellTypeValName LIKE '%Non-Invasive BP Diastolic%' OR\n",
    "        chart.nursingChartCellTypeVallabel LIKE '%Temperature (%' OR\n",
    "        chart.nursingChartCellTypeValName LIKE '%Saturation%' \n",
    "    )\n",
    "    AND chart.patientunitstayid IN (\n",
    "        SELECT\n",
    "            patient.patientunitstayid\n",
    "        FROM\n",
    "            eicu.patient patient\n",
    "        WHERE\n",
    "            patient.unitDischargeOffset / 1440.0 >= 1 \n",
    "            AND patient.unitDischargeOffset / 1440.0 < 60\n",
    "    );\n",
    "'''\n",
    "chart = pd.read_sql_query(query, conn)\n",
    "chart = chart[chart['stay_id'].isin(patients['stay_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart['label'] = chart['label'].replace({'Non-Invasive BP Systolic':'sbp',\n",
    "                                           'Non-Invasive BP Diastolic':'dbp',\n",
    "                                           'O2 Saturation':'SpO2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 화씨 온도를 섭씨 온도로 변환\n",
    "fahrenheit_mask = chart['label'] == 'Temperature (F)'\n",
    "celcius_mask = chart['label'] == 'Temperature (C)'\n",
    "chart.loc[fahrenheit_mask, 'value'] = (chart.loc[fahrenheit_mask, 'value'] - 32) * 5.0 / 9.0\n",
    "\n",
    "# label 수정\n",
    "chart.loc[fahrenheit_mask, 'label'] = 'Temperature'\n",
    "chart.loc[celcius_mask, 'label'] = 'Temperature'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vital sign 이상치 조정 \n",
    "\n",
    "chart['value'].replace('', np.nan, inplace=True)\n",
    "chart['value'] = chart['value'].astype(float)\n",
    "\n",
    "# 기준 설정\n",
    "ranges = {\n",
    "    'Respiratory Rate': (5, 50),\n",
    "    'Heart Rate': (10, 190),\n",
    "    'sbp': (40, 230),\n",
    "    'dbp': (20, 130),\n",
    "    'Temperature': (32, 41), \n",
    "    'SpO2': (68, 100)\n",
    "}\n",
    "# 각 label에 대한 평균값 계산\n",
    "mean_values = chart.groupby('label')['value'].mean()\n",
    "\n",
    "# 범위를 벗어난 값들을 NaN으로 대체\n",
    "def replace_out_of_range_with_nan(row):\n",
    "    label = row['label']\n",
    "    value = row['value']\n",
    "    \n",
    "    if label in ranges:\n",
    "        lower, upper = ranges[label]\n",
    "        \n",
    "        # 값이 범위를 벗어나면 NaN으로 대체\n",
    "        if value < lower or value > upper:\n",
    "            return np.nan\n",
    "    return value\n",
    "\n",
    "chart['value'] = chart.apply(replace_out_of_range_with_nan, axis=1)\n",
    "chart['value'] = chart.groupby('label')['value'].transform(lambda x: x.fillna(x.mean()))\n",
    "chart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### physical Exam: GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    exam.patientUnitStayID AS stay_id,\n",
    "    exam.physicalexampath AS label,\n",
    "    exam.physicalExamOffset / 60 AS time,\n",
    "    exam.physicalExamText AS value\n",
    "FROM\n",
    "    eicu.physicalExam exam\n",
    "WHERE\n",
    "    exam.physicalexampath LIKE '%Eyes Score%' OR\n",
    "    exam.physicalexampath LIKE '%Verbal Score%' OR\n",
    "    exam.physicalexampath LIKE '%Motor Score%';\n",
    "'''\n",
    "exam = pd.read_sql_query(query, conn)\n",
    "exam = exam[exam['stay_id'].isin(patients['stay_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam['label'] = exam['label'].replace(r'notes/Progress Notes/Physical Exam/Physical Exam/Neurologic/GCS/', '', regex=True)\n",
    "exam['label'] = exam['label'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
    "exam['value'] = pd.to_numeric(exam['value'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vital sign 이상치 조정 \n",
    "    \n",
    "ranges = {           \n",
    "    'Eyes Score':(1, 4),\n",
    "    'Verbal Score':(1, 5),\n",
    "    'Motor Score':(1, 6)}\n",
    "\n",
    "# 각 label에 대한 평균값 계산\n",
    "mean_values = exam.groupby('label')['value'].mean()\n",
    "\n",
    "# 범위를 벗어난 값들을 NaN으로 대체\n",
    "def replace_out_of_range_with_nan(row):\n",
    "    label = row['label']\n",
    "    value = row['value']\n",
    "    \n",
    "    if label in ranges:\n",
    "        lower, upper = ranges[label]\n",
    "        \n",
    "        # 값이 범위를 벗어나면 NaN으로 대체\n",
    "        if value < lower or value > upper:\n",
    "            return np.nan\n",
    "    return value\n",
    "\n",
    "exam['value'] = exam.apply(replace_out_of_range_with_nan, axis=1)\n",
    "exam['value'] = exam.groupby('label')['value'].transform(lambda x: x.fillna(x.mean()))\n",
    "exam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam.to_parquet('eICU_exam.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concat, pivot and merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([chart_1, chart_2, lab, exam, vitalap])\n",
    "\n",
    "df.drop(columns='valueoum', axis=1, inplace=True)\n",
    "df.to_parquet('eICU_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''chart_1 = pd.read_parquet('eICU_chart_1.parquet')\n",
    "chart_2 = pd.read_parquet('eICU_chart_2.parquet')\n",
    "lab = pd.read_parquet('eICU_lab.parquet')\n",
    "exam = pd.read_parquet('eICU_exam.parquet')\n",
    "vitalap = pd.read_parquet('eICU_vitalap.parquet')'''\n",
    "patients = pd.read_parquet('eICU_patients.parquet')\n",
    "df = pd.read_parquet('eICU_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['stay_id'].unique()) - set(patients['stay_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['time'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 921. MiB for an array with shape (120725664,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_chart \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstay_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df_chart\u001b[38;5;241m.\u001b[39mpivot_table(index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m], columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, aggfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/reshape/merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[1;32m    171\u001b[0m         left_df,\n\u001b[1;32m    172\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    183\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/reshape/merge.py:886\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m--> 886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[1;32m    889\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    890\u001b[0m )\n\u001b[1;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1151\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[1;32m   1148\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1151\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1125\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# make mypy happy\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masof\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_join_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1759\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[0;34m(left_keys, right_keys, sort, how)\u001b[0m\n\u001b[1;32m   1757\u001b[0m     _, lidx, ridx \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39mjoin(right, how\u001b[38;5;241m=\u001b[39mhow, return_indexers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1759\u001b[0m     lidx, ridx \u001b[38;5;241m=\u001b[39m \u001b[43mget_join_indexers_non_unique\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lidx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_range_indexer(lidx, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[1;32m   1764\u001b[0m     lidx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1799\u001b[0m, in \u001b[0;36mget_join_indexers_non_unique\u001b[0;34m(left, right, sort, how)\u001b[0m\n\u001b[1;32m   1797\u001b[0m     ridx, lidx \u001b[38;5;241m=\u001b[39m libjoin\u001b[38;5;241m.\u001b[39mleft_outer_join(rkey, lkey, count, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1799\u001b[0m     lidx, ridx \u001b[38;5;241m=\u001b[39m \u001b[43mlibjoin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1801\u001b[0m     lidx, ridx \u001b[38;5;241m=\u001b[39m libjoin\u001b[38;5;241m.\u001b[39mfull_outer_join(lkey, rkey, count)\n",
      "File \u001b[0;32mjoin.pyx:83\u001b[0m, in \u001b[0;36mpandas._libs.join.inner_join\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 921. MiB for an array with shape (120725664,) and data type int64"
     ]
    }
   ],
   "source": [
    "df_chart = df.merge(patients, on='stay_id', how='inner') \n",
    "df = df_chart.pivot_table(index=['stay_id', 'time'], columns='label', values='value', aggfunc='first')\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dead in hosp\n",
       "0    0.755387\n",
       "1    0.244613\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dead in hosp'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleansing and fillna\n",
    "- 결측치 처리 \n",
    "- 컬럼명/순서 수정\n",
    "- 날짜 컬럼 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('eICU_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stay_id                0.000000\n",
      "label                  0.000000\n",
      "time                   0.000000\n",
      "age                    0.000000\n",
      "gender                 0.000000\n",
      "intime                 0.000000\n",
      "CU type                0.000000\n",
      "race                   0.000000\n",
      "dead in hosp           0.000000\n",
      "death or dischtime     0.000000\n",
      "value                  0.132233\n",
      "valueoum              94.815052\n",
      "labtype               94.815052\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 각 컬럼의 결측치 비율 계산\n",
    "    \n",
    "def missing_value_ratio(df):\n",
    "    missing_ratio = df.isna().mean() * 100\n",
    "    \n",
    "    # 결측치 비율을 오름차순으로 정렬하여 출력)\n",
    "    print(missing_ratio.sort_values(ascending=True))\n",
    "\n",
    "missing_value_ratio(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert [['Heart Rate' 'Heart Rate' 'Heart Rate' ... 'dbp' 'dbp' 'dbp']] to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     df_ffill[selected_columns] \u001b[38;5;241m=\u001b[39m df_ffill[selected_columns]\u001b[38;5;241m.\u001b[39mfillna(overall_median_values)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_ffill\n\u001b[0;32m---> 16\u001b[0m df_fillna \u001b[38;5;241m=\u001b[39m \u001b[43mnan_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mnan_fill\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      8\u001b[0m df_ffill[selected_columns] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[selected_columns]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m group: group\u001b[38;5;241m.\u001b[39mffill())\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 2. median fill\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m overall_median_values \u001b[38;5;241m=\u001b[39m \u001b[43mdf_ffill\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m df_ffill[selected_columns] \u001b[38;5;241m=\u001b[39m df_ffill[selected_columns]\u001b[38;5;241m.\u001b[39mfillna(overall_median_values)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_ffill\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/frame.py:11706\u001b[0m, in \u001b[0;36mDataFrame.median\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11698\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m  11699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmedian\u001b[39m(\n\u001b[1;32m  11700\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11704\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11705\u001b[0m ):\n\u001b[0;32m> 11706\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[1;32m  11708\u001b[0m         result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/generic.py:12431\u001b[0m, in \u001b[0;36mNDFrame.median\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmedian\u001b[39m(\n\u001b[1;32m  12425\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12426\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12429\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12432\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmedian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  12433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12373\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_func(name, (), kwargs)\n\u001b[1;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m  12379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/frame.py:11562\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m  11558\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m  11560\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[1;32m  11561\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[0;32m> 11562\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  11563\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(res, axes\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m  11564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/internals/managers.py:1500\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1498\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m-> 1500\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[1;32m   1503\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/internals/blocks.py:404\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    407\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/frame.py:11481\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[0;34m(values, axis)\u001b[0m\n\u001b[1;32m  11479\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([result])\n\u001b[1;32m  11480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m> 11481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    145\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/home/mdhi/anaconda3/envs/predict/lib/python3.10/site-packages/pandas/core/nanops.py:787\u001b[0m, in \u001b[0;36mnanmedian\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    785\u001b[0m     inferred \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(values)\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inferred \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 787\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert [['Heart Rate' 'Heart Rate' 'Heart Rate' ... 'dbp' 'dbp' 'dbp']] to numeric"
     ]
    }
   ],
   "source": [
    "# stay_id별로 결측치 처리 \n",
    "\n",
    "def nan_fill(df):\n",
    "    # 1. forward fill\n",
    "    # 일부 컬럼은 제외 \n",
    "    columns_to_exclude = ['stay_id', 'gender', 'age', 'race', 'CU type', 'intime', 'death or dischtime', 'dead in hosp', 'time']\n",
    "    selected_columns = df.columns.difference(columns_to_exclude)\n",
    "    df_ffill = df.copy()\n",
    "    df_ffill[selected_columns] = df.groupby('stay_id')[selected_columns].apply(lambda group: group.ffill()).reset_index(drop=True)\n",
    "    \n",
    "    # 2. median fill\n",
    "    overall_median_values = df_ffill[selected_columns].median()\n",
    "    df_ffill[selected_columns] = df_ffill[selected_columns].fillna(overall_median_values)\n",
    "\n",
    "    return df_ffill\n",
    "\n",
    "df_fillna = nan_fill(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fillna['dead in hosp'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 퇴실 또는 사망 직전 1시간 이내의 측정값이 있는지를 확인하기 위해, 'last time' 컬럼을 생성하였음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fillna['intime'] = pd.to_datetime(df_fillna['intime'])\n",
    "df_fillna['charttime'] = df_fillna['intime'] + pd.to_timedelta(df_fillna['time'], unit='h')\n",
    "\n",
    "# 'charttime'에서 시간만 추출하여 새로운 컬럼에 저장\n",
    "df_fillna['charttime_time'] = df_fillna['charttime'].dt.time\n",
    "\n",
    "def time_str_to_timedelta(time_str):\n",
    "    if isinstance(time_str, str):\n",
    "        time_obj = datetime.strptime(time_str, '%H:%M:%S').time()\n",
    "    elif isinstance(time_str, time):\n",
    "        time_obj = time_str\n",
    "    else:\n",
    "        return pd.NaT  # time_str이 문자열이나 time 객체가 아니면 NaT 반환\n",
    "    \n",
    "    return timedelta(hours=time_obj.hour, minutes=time_obj.minute, seconds=time_obj.second)\n",
    "\n",
    "# 'death or dischtime'과 'charttime_time' 컬럼을 timedelta로 변환\n",
    "df_fillna['death_or_dischtime_time'] = df_fillna['death or dischtime'].apply(time_str_to_timedelta)\n",
    "df_fillna['charttime_time'] = df_fillna['charttime_time'].apply(time_str_to_timedelta)\n",
    "df_fillna['time_difference'] = df_fillna['death_or_dischtime_time'] - df_fillna['charttime_time']\n",
    "\n",
    "# 'hours' 열을 추출\n",
    "df_fillna['last time'] = df_fillna['time_difference'].apply(lambda x: x.total_seconds() / 3600 if pd.notnull(x) else pd.NaT)\n",
    "df_fillna.drop(columns=['death_or_dischtime_time', 'charttime_time', 'time_difference'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fillna = pd.read_parquet('eICU_df_fillna.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg/pos 비율 확인 \n",
    "\n",
    "df_fillna['dead in hosp'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stay_id별로 가장 최근 행만 먼저 추출\n",
    "df_fillna_max_time = df_fillna.loc[df_fillna.groupby('stay_id')['time'].idxmax()]\n",
    "\n",
    "# 해당 행 중에서 hours 컬럼의 값이 0 이상 1 미만인 경우만 필터링\n",
    "filtered_df_fillna = df_fillna_max_time[(df_fillna_max_time['hours'] >= 0) & (df_fillna_max_time['hours'] < 1)]\n",
    "filtered_df_fillna_list = filtered_df_fillna['stay_id'].unique()\n",
    "df_fillna_filtered = df_fillna[df_fillna['stay_id'].isin(filtered_df_fillna_list)]\n",
    "df_fillna_filtered.drop(columns=['hours', 'time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg/pos 비율 확인 \n",
    "\n",
    "df_fillna_filtered['dead in hosp'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_group(group):\n",
    "    # 'charttime'을 datetime 인덱스로 설정\n",
    "    group = group.set_index('charttime')\n",
    "    \n",
    "    # float형 데이터만 선택하여 평균 계산\n",
    "    numeric_cols = group.select_dtypes(include=['float']).columns\n",
    "    group_resampled = group[numeric_cols].resample('h').mean()\n",
    "    \n",
    "    # int, object형 데이터는 해당 stay_id의 전후값으로 계산\n",
    "    non_numeric_cols = group.select_dtypes(exclude=['float']).columns\n",
    "    group_non_numeric_resampled = group[non_numeric_cols].resample('h').ffill().bfill()\n",
    "    \n",
    "    # 리샘플링된 numeric 데이터와 non-numeric 데이터를 병합\n",
    "    group_resampled = group_resampled.join(group_non_numeric_resampled)\n",
    "    \n",
    "    # stay_id 다시 설정\n",
    "    group_resampled['stay_id'] = group['stay_id'].iloc[0]\n",
    "    \n",
    "    return group_resampled.reset_index()\n",
    "\n",
    "def resampling(df):\n",
    "    resampled = df.groupby('stay_id').apply(resample_group).reset_index(drop=True)\n",
    "    resampled.sort_values(by=['stay_id', 'charttime'], ascending=True, inplace=True)\n",
    "    return resampled\n",
    "\n",
    "# 리샘플링 함수 호출\n",
    "df_filtered_resample = resampling(df_fillna_filtered)\n",
    "df_filtered_resample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stay_id별 데이터 개수 계산\n",
    "group_counts = df_filtered_resample.groupby('stay_id').size()\n",
    "valid_stay_ids = group_counts[group_counts >= 24].index\n",
    "\n",
    "# 원래 데이터프레임에서 24개 이상의 데이터를 가지는 stay_id 그룹만 선택\n",
    "df_fillna_24 = df_filtered_resample[df_filtered_resample['stay_id'].isin(valid_stay_ids)]\n",
    "df_fillna_24.sort_values(by=['stay_id', 'charttime'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 stay_id group이 1시간 간격의 데이터를 가지고 있는지 확인\n",
    "\n",
    "def check_hourly_intervals(df):\n",
    "    groups_with_problems = []\n",
    "\n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        group = group.sort_values('charttime')\n",
    "        time_diffs = group['charttime'].diff().dropna()\n",
    "        \n",
    "        if not all(time_diffs == pd.Timedelta(hours=1)):\n",
    "            groups_with_problems.append(stay_id)\n",
    "    \n",
    "    return groups_with_problems\n",
    "\n",
    "len(check_hourly_intervals(df_fillna_24)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg/pos 비율 확인 \n",
    "\n",
    "df_fillna_24['dead in hosp'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos와 neg 분리 \n",
    "\n",
    "df_pos = df_fillna_24[df_fillna_24['dead in hosp'] == 1]\n",
    "df_neg = df_fillna_24[df_fillna_24['dead in hosp'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive의 경우 각 stay_id별로 최신 시간을 기준으로 이전 24개의 행만 남김\n",
    "\n",
    "def keep_last_24_hours(df):\n",
    "    # 최신 charttime을 기준으로 정렬\n",
    "    df_sorted = df.sort_values(by='charttime', ascending=True)\n",
    "    df_kept = df_sorted.groupby('stay_id').tail(24)\n",
    "    df_kept.sort_values(by=['stay_id', 'charttime'], ascending=True)\n",
    "    \n",
    "    return df_kept\n",
    "\n",
    "df_pos_24 = keep_last_24_hours(df_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative의 경우 각 stay_id에서 일부 시점을 무작위로 선택해 해당 시점 전후의 값만 사용\n",
    "\n",
    "def keep_random_24_hours(df):\n",
    "    np.random.seed(42)  # 시드 고정\n",
    "    \n",
    "    def select_random_window(group):\n",
    "        group_sorted = group.sort_values(by='charttime', ascending=False)\n",
    "        random_index = np.random.randint(0, len(group_sorted))\n",
    "\n",
    "        start_index = max(0, random_index - 11)\n",
    "        end_index = min(len(group_sorted), random_index + 12 + 1)\n",
    "        selected_window = group_sorted.iloc[start_index:end_index]\n",
    "\n",
    "        if len(selected_window) < 24:\n",
    "            if start_index == 0:\n",
    "                additional_rows = group_sorted.iloc[end_index:end_index + (24 - len(selected_window))]\n",
    "            else:\n",
    "                additional_rows = group_sorted.iloc[max(0, start_index - (24 - len(selected_window))):start_index]\n",
    "            selected_window = pd.concat([selected_window, additional_rows]).drop_duplicates().head(24)\n",
    "\n",
    "        return selected_window\n",
    "    \n",
    "    # 각 stay_id 그룹에 대해 select_random_window 함수를 적용\n",
    "    df_kept = df.groupby('stay_id').apply(select_random_window).reset_index(drop=True)\n",
    "    df_kept = df_kept.sort_values(by=['stay_id','charttime'], ascending=True)\n",
    "    \n",
    "    return df_kept\n",
    "\n",
    "df_neg_24 = keep_random_24_hours(df_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg 비율을 전체의 10%로 언더샘플링 > neg:pos 비율을 4:1로 맞춤\n",
    "\n",
    "neg_undersampling = df_neg_24['stay_id'].drop_duplicates().sample(frac=0.1, random_state=42)\n",
    "df_neg_24_under = df_neg_24[df_neg_24['stay_id'].isin(neg_undersampling)]\n",
    "\n",
    "df_24 = pd.concat([df_pos_24, df_neg_24_under])\n",
    "df_24.sort_values(by=['stay_id', 'charttime'], ascending=True, inplace=True)\n",
    "df_24['dead in hosp'].value_counts(normalize=True)\n",
    "\n",
    "df_24_fillna = nan_fill(df_24)\n",
    "df_24_fillna['dead in hosp'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime 변수 핸들링\n",
    "    \n",
    "def datetime_remove(df):\n",
    "    df_no_dates = df.drop(columns=['intime', 'death or dischtime'])\n",
    "    df_no_dates = df_no_dates.sort_values(by=['stay_id', 'charttime'], ascending=True)\n",
    "    return df_no_dates\n",
    "\n",
    "df_24 = datetime_remove(df_24_fillna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_24 = df_24[df_24['gender'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 범주형 열 목록\n",
    "categorical_cols = ['race', 'gender', 'CU type']\n",
    "\n",
    "# 범주형 변수 원핫인코딩\n",
    "df_encoded = pd.get_dummies(df_24, columns=categorical_cols)\n",
    "df_encoded['dead in hosp'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 개형 수정\n",
    "- 더미 데이터 추가\n",
    "- 컬럼 순서 재정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['CU type_CVICU'] = False\n",
    "df_encoded['PT'] = 4.0\n",
    "df_encoded = df_encoded[df_encoded['CU type_Neuro ICU'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded[['charttime', 'stay_id', 'ALT', 'AST', 'albumin', 'BUN', 'CRP',\n",
    "       'chloride', 'creatinine', 'Eyes Score', 'Motor Score',\n",
    "       'Verbal Score', 'glucose', 'Heart Rate', 'Hgb',\n",
    "       'platelets', 'potassium', 'PT', 'Respiratory Rate',\n",
    "       'sodium', 'SpO2', 'Temperature', 'WBC', 'dbp', 'sbp', 'age', \n",
    "       'race_asian', 'race_black', 'race_hispanic/latino',\n",
    "       'race_other', 'race_white', 'gender_Female', 'gender_Male', 'CU type_CCU',\n",
    "       'CU type_CVICU', 'CU type_MICU', 'CU type_SICU',\n",
    "       'dead in hosp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X, y split, 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vital sign 이상치 조정 \n",
    "\n",
    "# 기준 설정\n",
    "ranges = {\n",
    "    'Respiratory Rate': (5, 50),\n",
    "    'Heart Rate': (10, 190),\n",
    "    'sbp': (40, 230),\n",
    "    'dbp': (20, 130),\n",
    "    'Temperature': (32, 41), \n",
    "    'SpO2': (68, 100),    \n",
    "    'GCS Eye Opening':(1, 4),\n",
    "    'GCS Verbal Response':(1, 5),\n",
    "    'GCS Motor Response':(1, 6),\n",
    "        \n",
    "    'ALT':(0, 5000), \n",
    "    'AST':(0, 10000), \n",
    "    'Albumin':(0, 5.5), \n",
    "    'BUN':(0, 150), \n",
    "    'Bilirubin':(0, 50),\n",
    "    'CRP':(0, 300), \n",
    "    'Chloride':(70, 135),\n",
    "    'Creatinine':(0, 15),\n",
    "    'Glucose':(0, 600),\n",
    "    'Hemoglobin':(0, 25),\n",
    "    'Potassium':(2, 9),\n",
    "    'Sodium':(105, 170),\n",
    "    'Platelet':(0, 1000),\n",
    "    'Prothrombin time':(0, 8), \n",
    "    'WBC':(0, 90), \n",
    "}\n",
    "# 각 열에 대해 이상치를 NaN으로 대체\n",
    "for col, (lower, upper) in ranges.items():\n",
    "    if col in df_encoded.columns:\n",
    "        df_encoded[col] = np.where((df_encoded[col] < lower) | (df_encoded[col] > upper), np.nan, df_encoded[col])\n",
    "\n",
    "# NaN 값을 각 열에 대한 평균값으로 대체\n",
    "for col in ranges.keys():\n",
    "    if col in df_encoded.columns:\n",
    "        df_encoded[col] = df_encoded[col].fillna(df_encoded[col].mean())\n",
    "\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = df_encoded['dead in hosp']\n",
    "X_val = df_encoded.drop(columns=['dead in hosp'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(df):\n",
    "    # 'charttime'을 인덱스로 설정\n",
    "    df.set_index('charttime', inplace=True)\n",
    "    df.drop(columns=['stay_id'], inplace=True)\n",
    "    \n",
    "    # 정규화\n",
    "    scaler = StandardScaler() # MinMaxScaler / StandardScaler\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "X_val_scaled = scaling(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_ratio(df):\n",
    "    # 클래스별 개수 계산\n",
    "    class_counts = df.value_counts()\n",
    "\n",
    "    # 전체 데이터 개수\n",
    "    total_count = len(df)\n",
    "\n",
    "    # 클래스별 비율 계산\n",
    "    class_ratios = class_counts / total_count\n",
    "\n",
    "    print(\"클래스 0의 비율:\", class_ratios[0])\n",
    "    print(\"클래스 1의 비율:\", class_ratios[1])\n",
    "\n",
    "# 비율 유지되는지 확인 \n",
    "\n",
    "class_ratio(df_encoded['dead in hosp'])\n",
    "class_ratio(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_val_scaled.shape[1]\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_parquet('eICU_valid_dataset.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
